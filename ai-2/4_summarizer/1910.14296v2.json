{
    "extracted_text": "arXiv:1910.14296v2 [cs.CL] 6 Oct 2020\nLIMIT-BERT: Linguistics Informed Multi-Task BERT\n1,2,3\nJunru Zhou1,2,3, Zhuosheng Zhang 1,2,3, Hai Zhao1,2,3*, Shuailiang Zhang\n1\n¹Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3 MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n{zhoujunru, zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nIn this paper, we present Linguistics Informed\nMulti-Task BERT (LIMIT-BERT) for learning\nlanguage representations across multiple lin-\nguistics tasks by Multi-Task Learning. LIMIT-\nBERT includes five key linguistics tasks: Part-\nOf-Speech (POS) tags, constituent and de-\npendency syntactic parsing, span and depen-\ndency semantic role labeling (SRL). Differ-\nent from recent Multi-Task Deep Neural Net-\nworks (MT-DNN), our LIMIT-BERT is fully\nlinguistics motivated and thus is capable of\nadopting an improved masked training objec-\ntive according to syntactic and semantic con-\nstituents. Besides, LIMIT-BERT takes a semi-\nsupervised learning strategy to offer the same\nlarge amount of linguistics task data as that\nfor the language model training. As a re-\nsult, LIMIT-BERT not only improves linguis-\ntics tasks performance, but also benefits from\na regularization effect and linguistics infor-\nmation that leads to more general representa-\ntions to help adapt to new tasks and domains.\nLIMIT-BERT outperforms the strong baseline\nWhole Word Masking BERT on both depen-\ndency and constituent syntactic/semantic pars-\ning, GLUE benchmark, and SNLI task. Our\npractice on the proposed LIMIT-BERT also en-\nables us to release a well pre-trained model for\nmulti-purpose of natural language processing\ntasks once for all.\n1 Introduction\nRecently, pre-trained language models have shown\ngreatly effective across a range of linguistics in-\nspired natural language processing (NLP) tasks\nsuch as syntactic parsing, semantic parsing and\n*Corresponding author. This paper was partially sup-\nported by National Key Research and Development Program\nof China (No. 2017YFB0304100), Key Projects of Na-\ntional Natural Science Foundation of China (U1836222 and\n61733011), Huawei-SJTU long term AI project, Cutting-edge\nMachine reading comprehension and language model.\nso on (Zhou and Zhao, 2019; Zhou et al., 2020;\nOuchi et al., 2018; He et al., 2018b; Li et al., 2019),\nwhen taking the latter as downstream tasks for\nthe former. In the meantime, introducing linguis-\ntic clues such as syntax and semantics into the\npre-trained language models may furthermore en-\nhance other downstream tasks such as various Nat-\nural Language Understanding (NLU) tasks (Zhang\net al., 2020a,b). However, nearly all existing lan-\nguage models are usually trained on large amounts\nof unlabeled text data (Peters et al., 2018; Devlin\net al., 2019), without explicitly exploiting linguis-\ntic knowledge. Such observations motivate us to\njointly consider both types of tasks, pre-training\nlanguage models, and solving linguistics inspired\nNLP problems. We argue such a treatment may\nbenefit from two-fold. (1) Joint learning is a better\nway to let the former help the latter in a bidirec-\ntional mode, rather than in a unidirectional mode,\ntaking the latter as downstream tasks of the former.\n(2) Naturally empowered by linguistic clues from\njoint learning, pre-trained language models will be\nmore powerful for enhancing downstream tasks.\nThus we propose Linguistics Informed Multi-Task\nBERT (LIMIT-BERT), making an attempt to in-\ncorporate linguistic knowledge into pre-training\nlanguage representation models. The proposed\nLIMIT-BERT is implemented in terms of Multi-\nTask Learning (MTL) (Caruana, 1993) which has\nshown useful, by alleviating overfitting to a spe-\ncific task, thus making the learned representations\nuniversal across tasks.\nSince universal language representations are\nlearning by leveraging large amounts of unlabeled\ndata which has quite different data volume com-\npared with linguistics tasks dataset such as Penn\nTreebank (PTB)¹ (Marcus et al., 1993).\nTo alleviate such data unbalance on multi-task\n'PTB is an English treebank with syntactic tree annotation\nwhich only contains 50k sentences."
}